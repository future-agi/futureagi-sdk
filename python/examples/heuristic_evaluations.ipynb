{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Install the package from parent directory\n",
    "subprocess.check_call([\"pip\", \"install\", \"-e\", \"..\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fi.evals.metrics import BLEUScore\n",
    "from fi.testcases import TestCase\n",
    "\n",
    "def test_bleu_configurations():\n",
    "    test_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "    reference = \"The fast brown fox leaps over the sleepy dog\"\n",
    "    \n",
    "    # 1. Default configuration (sentence mode)\n",
    "    default_bleu = BLEUScore()\n",
    "    \n",
    "    # 2. Sentence mode with custom n-gram\n",
    "    bigram_bleu = BLEUScore(config={\n",
    "        \"mode\": \"sentence\",\n",
    "        \"max_n_gram\": 2,  # Use only unigrams and bigrams\n",
    "        \"smooth\": \"method1\"\n",
    "    })\n",
    "    \n",
    "    # 3. Corpus mode\n",
    "    corpus_bleu = BLEUScore(config={\n",
    "        \"mode\": \"corpus\",\n",
    "        \"max_n_gram\": 4,\n",
    "        \"smooth\": \"method1\"\n",
    "    })\n",
    "    \n",
    "    # 4. Custom weights for different n-grams\n",
    "    weighted_bleu = BLEUScore(config={\n",
    "        \"mode\": \"sentence\",\n",
    "        \"weights\": [0.4, 0.3, 0.2, 0.1],  # More weight on unigrams, less on 4-grams\n",
    "        \"smooth\": \"method1\"\n",
    "    })\n",
    "    \n",
    "    # 5. Different smoothing method\n",
    "    smooth2_bleu = BLEUScore(config={\n",
    "        \"mode\": \"sentence\",\n",
    "        \"max_n_gram\": 4,\n",
    "        \"smooth\": \"method2\"\n",
    "    })\n",
    "    \n",
    "    test_case = TestCase(\n",
    "        response=test_text,\n",
    "        expected_text=reference\n",
    "    )\n",
    "    \n",
    "    configs = {\n",
    "        \"Default\": default_bleu,\n",
    "        \"Bigram Only\": bigram_bleu,\n",
    "        \"Corpus Mode\": corpus_bleu,\n",
    "        \"Custom Weights\": weighted_bleu,\n",
    "        \"Method2 Smoothing\": smooth2_bleu\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, evaluator in configs.items():\n",
    "        result = evaluator.evaluate([test_case])\n",
    "        results[name] = result.eval_results[0].metrics[0].value\n",
    "    \n",
    "    for name, score in results.items():\n",
    "        print(f\"{name}: {score:.4f}\")\n",
    "    \n",
    "\n",
    "test_bleu_configurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fi.evals.metrics import ROUGEScore\n",
    "from fi.testcases import TestCase\n",
    "\n",
    "def test_rouge_configurations():\n",
    "    test_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "    reference = \"The fast brown fox leaps over the sleepy dog\"\n",
    "    \n",
    "    # 1. Default configuration (rouge1)\n",
    "    default_rouge = ROUGEScore()\n",
    "    \n",
    "    # 2. Rouge2 (bigram matching)\n",
    "    rouge2 = ROUGEScore(config={\n",
    "        \"rouge_type\": \"rouge2\",\n",
    "        \"use_stemmer\": True\n",
    "    })\n",
    "    \n",
    "    # 3. RougeL (longest common subsequence)\n",
    "    rougeL = ROUGEScore(config={\n",
    "        \"rouge_type\": \"rougeL\",\n",
    "        \"use_stemmer\": True\n",
    "    })\n",
    "    \n",
    "    # 4. Rouge1 without stemming\n",
    "    rouge1_no_stem = ROUGEScore(config={\n",
    "        \"rouge_type\": \"rouge1\",\n",
    "        \"use_stemmer\": False\n",
    "    })\n",
    "    \n",
    "    test_case = TestCase(\n",
    "        response=test_text,\n",
    "        expected_text=reference\n",
    "    )\n",
    "    \n",
    "    configs = {\n",
    "        \"Default (Rouge1)\": default_rouge,\n",
    "        \"Rouge2\": rouge2,\n",
    "        \"RougeL\": rougeL,\n",
    "        \"Rouge1 No Stemming\": rouge1_no_stem\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, evaluator in configs.items():\n",
    "        result = evaluator.evaluate([test_case])\n",
    "        scores = {\n",
    "            \"precision\": result.eval_results[0].metrics[0].value,\n",
    "            \"recall\": result.eval_results[0].metrics[1].value,\n",
    "            \"fmeasure\": result.eval_results[0].metrics[2].value\n",
    "        }\n",
    "        results[name] = scores\n",
    "    \n",
    "    for name, scores in results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "        print(f\"  Recall: {scores['recall']:.4f}\")\n",
    "        print(f\"  F-measure: {scores['fmeasure']:.4f}\")\n",
    "    \n",
    "\n",
    "\n",
    "test_rouge_configurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fi.evals.metrics import EmbeddingSimilarity\n",
    "from fi.testcases import TestCase\n",
    "\n",
    "\n",
    "def test_embedding_similarity_configurations():\n",
    "    test_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "    reference = \"The fast brown fox leaps over the sleepy dog\"\n",
    "    \n",
    "    # 1. Default configuration (cosine similarity)\n",
    "    default_embedding = EmbeddingSimilarity()\n",
    "    \n",
    "    # 2. Euclidean distance\n",
    "    euclidean_embedding = EmbeddingSimilarity(config={\n",
    "        \"similarity_method\": \"euclidean\",\n",
    "        \"normalize\": True\n",
    "    })\n",
    "    \n",
    "    # 3. Manhattan distance\n",
    "    manhattan_embedding = EmbeddingSimilarity(config={\n",
    "        \"similarity_method\": \"manhattan\",\n",
    "        \"normalize\": True\n",
    "    })\n",
    "    \n",
    "    # 4. Cosine without normalization\n",
    "    cosine_no_norm = EmbeddingSimilarity(config={\n",
    "        \"similarity_method\": \"cosine\",\n",
    "        \"normalize\": False\n",
    "    })\n",
    "    \n",
    "    \n",
    "    test_case = TestCase(\n",
    "        response=test_text,\n",
    "        expected_text=reference\n",
    "    )\n",
    "    \n",
    "    configs = {\n",
    "        \"Default (Cosine)\": default_embedding,\n",
    "        \"Euclidean Distance\": euclidean_embedding,\n",
    "        \"Manhattan Distance\": manhattan_embedding,\n",
    "        \"Cosine No Normalization\": cosine_no_norm\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, evaluator in configs.items():\n",
    "        result = evaluator.evaluate([test_case])\n",
    "        results[name] = result.eval_results[0].metrics[0].value\n",
    "    \n",
    "    for name, score in results.items():\n",
    "        print(f\"{name}: {score:.4f}\")\n",
    "    \n",
    "\n",
    "test_embedding_similarity_configurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lavenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fi.evals.metrics import LevenshteinDistance\n",
    "from fi.testcases import TestCase\n",
    "\n",
    "def test_levenshtein_input_types():\n",
    "    # 1. Default configuration (case-sensitive, with punctuation)\n",
    "    default_levenshtein = LevenshteinDistance()\n",
    "    \n",
    "    # 2. Case-insensitive comparison\n",
    "    case_insensitive = LevenshteinDistance(config={\n",
    "        \"case_insensitive\": True\n",
    "    })\n",
    "    \n",
    "    # 3. Punctuation removal\n",
    "    no_punctuation = LevenshteinDistance(config={\n",
    "        \"remove_punctuation\": True\n",
    "    })\n",
    "    \n",
    "    # 4. Both case-insensitive and no punctuation\n",
    "    both_options = LevenshteinDistance(config={\n",
    "        \"case_insensitive\": True,\n",
    "        \"remove_punctuation\": True\n",
    "    })\n",
    "    \n",
    "    #various test cases\n",
    "    \n",
    "    # A. Standard text comparison\n",
    "    standard_case = TestCase(\n",
    "        response=\"The quick brown fox jumps over the lazy dog.\",\n",
    "        expected_text=\"The quick brown fox jumps over the lazy dog.\"\n",
    "    )\n",
    "    \n",
    "    # B. Case difference\n",
    "    case_difference = TestCase(\n",
    "        response=\"The Quick Brown Fox Jumps Over The Lazy Dog.\",\n",
    "        expected_text=\"the quick brown fox jumps over the lazy dog.\"\n",
    "    )\n",
    "    \n",
    "    # C. Punctuation difference\n",
    "    punctuation_difference = TestCase(\n",
    "        response=\"The quick brown fox jumps over the lazy dog!\",\n",
    "        expected_text=\"The quick brown fox jumps over the lazy dog.\"\n",
    "    )\n",
    "    \n",
    "    # D. Both case and punctuation differences\n",
    "    both_differences = TestCase(\n",
    "        response=\"The Quick Brown Fox Jumps Over the Lazy Dog!\",\n",
    "        expected_text=\"the quick brown fox jumps over the lazy dog.\"\n",
    "    )\n",
    "    \n",
    "    # E. Word order difference (which Levenshtein will penalize)\n",
    "    word_order = TestCase(\n",
    "        response=\"The dog lazy the over jumps fox brown quick.\",\n",
    "        expected_text=\"The quick brown fox jumps over the lazy dog.\"\n",
    "    )\n",
    "    \n",
    "    # F. Slightly different text\n",
    "    slight_difference = TestCase(\n",
    "        response=\"The quick brown fox leaps over the sleeping dog.\",\n",
    "        expected_text=\"The quick brown fox jumps over the lazy dog.\"\n",
    "    )\n",
    "    \n",
    "    # G. Special characters\n",
    "    special_chars = TestCase(\n",
    "        response=\"The quick brown fox â€” it jumps over the lazy dog!\",\n",
    "        expected_text=\"The quick brown fox (jumps) over the lazy dog.\"\n",
    "    )\n",
    "    \n",
    "    # H. Numeric content\n",
    "    numeric_content = TestCase(\n",
    "        response=\"The 5 quick foxes jump over 2 lazy dogs.\",\n",
    "        expected_text=\"The five quick foxes jump over two lazy dogs.\"\n",
    "    )\n",
    "    \n",
    "    # I. Empty strings\n",
    "    empty_response = TestCase(\n",
    "        response=\"\",\n",
    "        expected_text=\"The quick brown fox jumps over the lazy dog.\"\n",
    "    )\n",
    "    \n",
    "    test_cases = {\n",
    "        \"Standard\": standard_case,\n",
    "        \"Case Difference\": case_difference,\n",
    "        \"Punctuation Difference\": punctuation_difference,\n",
    "        \"Case & Punctuation\": both_differences,\n",
    "        \"Word Order\": word_order,\n",
    "        \"Slight Difference\": slight_difference,\n",
    "        \"Special Characters\": special_chars,\n",
    "        \"Numeric Content\": numeric_content,\n",
    "        \"Empty Response\": empty_response\n",
    "    }\n",
    "    \n",
    "    evaluators = {\n",
    "        \"Default\": default_levenshtein,\n",
    "        \"Case-Insensitive\": case_insensitive,\n",
    "        \"No Punctuation\": no_punctuation,\n",
    "        \"Case-Ins & No Punct\": both_options\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for eval_name, evaluator in evaluators.items():\n",
    "        results[eval_name] = {}\n",
    "        for case_name, test_case in test_cases.items():\n",
    "            result = evaluator.evaluate([test_case])\n",
    "            results[eval_name][case_name] = result.eval_results[0].metrics[0].value\n",
    "    \n",
    "    for eval_name, cases in results.items():\n",
    "        print(f\"\\n{eval_name} Configuration:\")\n",
    "        print(\"-\" * 40)\n",
    "        for case_name, score in cases.items():\n",
    "            print(f\"{case_name:20s}: {score:.4f}\")\n",
    "    \n",
    "\n",
    "test_levenshtein_input_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Diff Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fi.evals.metrics import NumericDiff\n",
    "from fi.testcases import TestCase\n",
    "\n",
    "def test_numeric_diff_comparison():\n",
    "    normalized_diff = NumericDiff(config={\n",
    "        \"extract_numeric\": True,\n",
    "        \"normalized_result\": True\n",
    "    })\n",
    "    \n",
    "    absolute_diff = NumericDiff(config={\n",
    "        \"extract_numeric\": True,\n",
    "        \"normalized_result\": False\n",
    "    })\n",
    "    \n",
    "    test_cases = [\n",
    "        # Basic cases\n",
    "        TestCase(response=\"100\", expected_text=\"100\"),\n",
    "        TestCase(response=\"50\", expected_text=\"100\"),\n",
    "        TestCase(response=\"150\", expected_text=\"100\"),\n",
    "        \n",
    "        # Text with numbers\n",
    "        TestCase(response=\"The price is 99.5 dollars\", expected_text=\"100\"),\n",
    "        TestCase(response=\"The measurement is 1.5 meters\", expected_text=\"The expected measurement was 1 meter\"),\n",
    "        \n",
    "        # Small and large numbers\n",
    "        TestCase(response=\"0.001\", expected_text=\"0.002\"),\n",
    "        TestCase(response=\"1000000\", expected_text=\"2000000\"),\n",
    "        \n",
    "        # Zero reference\n",
    "        TestCase(response=\"5\", expected_text=\"0\"),\n",
    "        \n",
    "        # Negative numbers\n",
    "        TestCase(response=\"-10\", expected_text=\"-8\")\n",
    "    ]\n",
    "    \n",
    "    # Run evaluations and show results\n",
    "    print(\"Numeric Difference Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Test Case':30s} | {'Normalized':10s} | {'Absolute':10s}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for tc in test_cases:\n",
    "        norm_result = normalized_diff.evaluate([tc])\n",
    "        abs_result = absolute_diff.evaluate([tc])\n",
    "        \n",
    "        norm_value = norm_result.eval_results[0].metrics[0].value\n",
    "        abs_value = abs_result.eval_results[0].metrics[0].value\n",
    "        \n",
    "        case_desc = f\"{tc.response}, {tc.expected_text}\"\n",
    "        if len(case_desc) > 28:\n",
    "            case_desc = case_desc[:25] + \"...\"\n",
    "            \n",
    "        print(f\"{case_desc:30s} | {norm_value:10.4f} | {abs_value:10.2f}\")\n",
    "    \n",
    "\n",
    "test_numeric_diff_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic List Contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fi.evals.metrics import SemanticListContains\n",
    "from fi.testcases import TestCase\n",
    "import json\n",
    "\n",
    "# 1. Single string expected_text\n",
    "single_string = TestCase(\n",
    "    response=\"The quick brown fox jumps over the lazy dog\",\n",
    "    expected_text=\"fox\"\n",
    ")\n",
    "\n",
    "# 2. JSON-encoded list of strings\n",
    "json_list = TestCase(\n",
    "    response=\"The quick brown fox jumps over the lazy dog\",\n",
    "    expected_text=json.dumps([\"brown fox\", \"lazy dog\"])\n",
    ")\n",
    "\n",
    "# 3. JSON-encoded list with mixed matches\n",
    "partial_match = TestCase(\n",
    "    response=\"The quick brown fox jumps over the lazy dog\",\n",
    "    expected_text=json.dumps([\"brown fox\", \"lazy dog\", \"flying elephant\"])\n",
    ")\n",
    "\n",
    "# 4. No matching phrases\n",
    "no_match = TestCase(\n",
    "    response=\"The quick brown fox jumps over the lazy dog\",\n",
    "    expected_text=json.dumps([\"flying elephant\", \"dancing giraffe\"])\n",
    ")\n",
    "\n",
    "# Create evaluator with default configuration\n",
    "evaluator = SemanticListContains(\n",
    "    config={\n",
    "        \"similarity_threshold\": 0.5\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test with different inputs\n",
    "for i, test_case in enumerate([single_string, json_list, partial_match, no_match], 1):\n",
    "    result = evaluator.evaluate([test_case])\n",
    "    score = result.eval_results[0].metrics[0].value\n",
    "    \n",
    "    metadata = result.eval_results[0].metadata\n",
    "    \n",
    "    print(f\"Response: {test_case.response}\")\n",
    "    print(f\"Expected Keywords: {test_case.expected_text}\")\n",
    "    print(score)\n",
    "    print(metadata)\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fi.evals.metrics import BLEUScore, ROUGEScore, LevenshteinDistance, AggregatedMetric\n",
    "from fi.testcases import TestCase\n",
    "\n",
    "# Create a standard test case\n",
    "test_case = TestCase(\n",
    "    response=\"The quick brown fox jumps over the lazy dog.\",\n",
    "    expected_text=\"The quick brown fox jumps over the lazy dog.\"\n",
    ")\n",
    "\n",
    "# Create individual metrics\n",
    "bleu = BLEUScore()\n",
    "rouge = ROUGEScore(config={\"rouge_type\": \"rouge1\"})\n",
    "levenshtein = LevenshteinDistance()\n",
    "\n",
    "# Example 1: BLEU and ROUGE with simple average\n",
    "avg_metric = AggregatedMetric(config={\n",
    "    \"metrics\": [bleu, rouge],\n",
    "    \"metric_names\": [\"bleu\", \"rouge1\"],\n",
    "    \"aggregator\": \"average\"\n",
    "})\n",
    "\n",
    "# Example 2: BLEU and ROUGE with weighted average (70% BLEU, 30% ROUGE)\n",
    "weighted_metric = AggregatedMetric(config={\n",
    "    \"metrics\": [bleu, rouge],\n",
    "    \"metric_names\": [\"bleu\", \"rouge1\"],\n",
    "    \"aggregator\": \"weighted_average\",\n",
    "    \"weights\": [0.7, 0.3]\n",
    "})\n",
    "\n",
    "# Example 3: Combining BLEU, ROUGE and Levenshtein with average\n",
    "combined_metric = AggregatedMetric(config={\n",
    "    \"metrics\": [bleu, rouge, levenshtein],\n",
    "    \"metric_names\": [\"bleu\", \"rouge1\", \"levenshtein\"],\n",
    "    \"aggregator\": \"average\"\n",
    "})\n",
    "\n",
    "print(\"BLEU + ROUGE with Simple Average\")\n",
    "result = avg_metric.evaluate([test_case])\n",
    "score = result.eval_results[0].metrics[0].value\n",
    "metrics = result.eval_results[0].metadata\n",
    "\n",
    "print(f\"Aggregated Score: {score:.4f}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"\\nBLEU + ROUGE with Weighted Average (70% BLEU, 30% ROUGE)\")\n",
    "result = weighted_metric.evaluate([test_case])\n",
    "score = result.eval_results[0].metrics[0].value\n",
    "metrics = result.eval_results[0].metadata\n",
    "\n",
    "print(f\"Aggregated Score: {score:.4f}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"\\nBLEU + ROUGE + Levenshtein with Average\")\n",
    "result = combined_metric.evaluate([test_case])\n",
    "score = result.eval_results[0].metrics[0].value\n",
    "metrics = result.eval_results[0].metadata\n",
    "\n",
    "print(f\"Aggregated Score: {score:.4f}\")\n",
    "print(\"--------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
