{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Install the package from parent directory\n",
    "subprocess.check_call([\"pip\", \"install\", \"-e\", \"..\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from fi.datasets import Dataset\n",
    "from fi.datasets.types import (\n",
    "    Cell,\n",
    "    Column,\n",
    "    DatasetConfig,\n",
    "    DataTypeChoices,\n",
    "    HuggingfaceDatasetConfig,\n",
    "    ModelTypes,\n",
    "    Row,\n",
    "    SourceChoices,\n",
    ")\n",
    "from fi.utils.errors import (\n",
    "    DatasetError,\n",
    "    DatasetNotFoundError,\n",
    "    DatasetValidationError,\n",
    "    MissingAuthError,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_environment():\n",
    "    \"\"\"Check if required environment variables are set\"\"\"\n",
    "    if not os.getenv(\"FI_API_KEY\") or not os.getenv(\"FI_SECRET_KEY\"):\n",
    "        raise MissingAuthError(\n",
    "            \"FI_API_KEY and FI_SECRET_KEY environment variables must be set\",\n",
    "            fi_secret_key=\"\"\n",
    "        )\n",
    "    print(os.getenv(\"FI_API_KEY\"))\n",
    "    print(os.getenv(\"FI_SECRET_KEY\"))\n",
    "    print(\"✓ Environment variables are set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### basic dataset lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_1_basic_dataset_lifecycle():\n",
    "    \"\"\"\n",
    "    Example 1: Basic Dataset Lifecycle\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Creating an empty dataset\n",
    "    - Adding columns with different data types\n",
    "    - Adding rows with data\n",
    "    - Downloading the dataset\n",
    "    - Deleting the dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Example 1: Basic Dataset Lifecycle ===\")\n",
    "    \n",
    "    # Create a unique dataset name\n",
    "    unique_name = f\"basic_example_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    # Step 1: Create dataset configuration\n",
    "    config = DatasetConfig(\n",
    "        name=unique_name,\n",
    "        model_type=ModelTypes.GENERATIVE_LLM\n",
    "    )\n",
    "    \n",
    "    # Step 2: Initialize dataset instance\n",
    "    dataset = Dataset(dataset_config=config)\n",
    "    \n",
    "    try:\n",
    "        # Step 3: Create empty dataset\n",
    "        print(f\"Creating dataset: {unique_name}\")\n",
    "        dataset = dataset.create()\n",
    "        print(f\"✓ Dataset created with ID: {dataset.dataset_config.id if dataset.dataset_config else 'Unknown'}\")\n",
    "        \n",
    "        # Step 4: Define columns with various data types\n",
    "        columns: List[Column] = [\n",
    "            Column(\n",
    "                name=\"user_query\",\n",
    "                data_type=DataTypeChoices.TEXT,\n",
    "                source=SourceChoices.OTHERS\n",
    "            ),\n",
    "            Column(\n",
    "                name=\"response_quality\",\n",
    "                data_type=DataTypeChoices.INTEGER,\n",
    "                source=SourceChoices.OTHERS\n",
    "            ),\n",
    "            Column(\n",
    "                name=\"is_helpful\",\n",
    "                data_type=DataTypeChoices.BOOLEAN,\n",
    "                source=SourceChoices.OTHERS\n",
    "            ),\n",
    "            Column(\n",
    "                name=\"response_time\",\n",
    "                data_type=DataTypeChoices.FLOAT,\n",
    "                source=SourceChoices.OTHERS\n",
    "            ),\n",
    "            Column(\n",
    "                name=\"metadata\",\n",
    "                data_type=DataTypeChoices.JSON,\n",
    "                source=SourceChoices.OTHERS\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Step 5: Add columns to dataset\n",
    "        print(\"Adding columns...\")\n",
    "        dataset = dataset.add_columns(columns=columns)  # type: ignore\n",
    "        print(\"✓ Columns added successfully\")\n",
    "        \n",
    "        # Step 6: Define sample data rows\n",
    "        rows: List[Row] = [\n",
    "            Row(\n",
    "                order=1,\n",
    "                cells=[\n",
    "                    Cell(column_name=\"user_query\", value=\"What is machine learning?\"),\n",
    "                    Cell(column_name=\"response_quality\", value=8),\n",
    "                    Cell(column_name=\"is_helpful\", value=True),\n",
    "                    Cell(column_name=\"response_time\", value=1.2),\n",
    "                    Cell(column_name=\"metadata\", value='{\"model\": \"gpt-4\", \"tokens\": 150}')\n",
    "                ]\n",
    "            ),\n",
    "            Row(\n",
    "                order=2,\n",
    "                cells=[\n",
    "                    Cell(column_name=\"user_query\", value=\"Explain quantum computing\"),\n",
    "                    Cell(column_name=\"response_quality\", value=9),\n",
    "                    Cell(column_name=\"is_helpful\", value=True),\n",
    "                    Cell(column_name=\"response_time\", value=2.1),\n",
    "                    Cell(column_name=\"metadata\", value='{\"model\": \"gpt-4\", \"tokens\": 200}')\n",
    "                ]\n",
    "            ),\n",
    "            Row(\n",
    "                order=3,\n",
    "                cells=[\n",
    "                    Cell(column_name=\"user_query\", value=\"What's the weather?\"),\n",
    "                    Cell(column_name=\"response_quality\", value=5),\n",
    "                    Cell(column_name=\"is_helpful\", value=False),\n",
    "                    Cell(column_name=\"response_time\", value=0.8),\n",
    "                    Cell(column_name=\"metadata\", value='{\"model\": \"gpt-3.5\", \"tokens\": 50}')\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Step 7: Add rows to dataset\n",
    "        print(\"Adding rows...\")\n",
    "        dataset = dataset.add_rows(rows=rows)  # type: ignore\n",
    "        print(\"✓ Rows added successfully\")\n",
    "        \n",
    "        # Step 8: Download dataset as CSV\n",
    "        output_file = f\"basic_example_{uuid.uuid4().hex[:8]}.csv\"\n",
    "        print(f\"Downloading dataset to {output_file}...\")\n",
    "        dataset.download(file_path=output_file)\n",
    "        print(f\"✓ Dataset downloaded to {output_file}\")\n",
    "        \n",
    "        # Step 9: Load as pandas DataFrame\n",
    "        print(\"Loading dataset as pandas DataFrame...\")\n",
    "        df = dataset.download(load_to_pandas=True)\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            print(f\"✓ Dataset loaded as DataFrame with shape: {df.shape}\")\n",
    "            print(\"First few rows:\")\n",
    "            print(df.head())\n",
    "        else:\n",
    "            print(\"✓ Dataset downloaded but not as DataFrame\")\n",
    "        \n",
    "        # Cleanup downloaded file\n",
    "        if os.path.exists(output_file):\n",
    "            os.remove(output_file)\n",
    "            print(f\"✓ Cleaned up {output_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in basic lifecycle: {e}\")\n",
    "    finally:\n",
    "        # Step 10: Delete dataset\n",
    "        try:\n",
    "            dataset.delete()\n",
    "            print(\"✓ Dataset deleted successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error deleting dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1_basic_dataset_lifecycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_2_create_from_file():\n",
    "    \"\"\"\n",
    "    Example 2: Creating Dataset from Local File\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Creating a sample CSV file\n",
    "    - Creating dataset from local file\n",
    "    - Working with file-based datasets\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Example 2: Creating Dataset from File ===\")\n",
    "    \n",
    "    # Step 1: Create sample CSV file\n",
    "    sample_data = {\n",
    "        \"question\": [\n",
    "            \"What is AI?\",\n",
    "            \"How does machine learning work?\",\n",
    "            \"What are neural networks?\"\n",
    "        ],\n",
    "        \"answer\": [\n",
    "            \"AI is artificial intelligence...\",\n",
    "            \"Machine learning uses algorithms...\",\n",
    "            \"Neural networks are computing systems...\"\n",
    "        ],\n",
    "        \"category\": [\"general\", \"technical\", \"technical\"],\n",
    "        \"difficulty\": [1, 3, 4]\n",
    "    }\n",
    "    \n",
    "    sample_file = f\"sample_data_{uuid.uuid4().hex[:8]}.csv\"\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv(sample_file, index=False)\n",
    "    print(f\"✓ Created sample file: {sample_file}\")\n",
    "    \n",
    "    # Step 2: Create dataset from file\n",
    "    unique_name = f\"file_example_{uuid.uuid4().hex[:8]}\"\n",
    "    config = DatasetConfig(\n",
    "        name=unique_name,\n",
    "        model_type=ModelTypes.GENERATIVE_LLM\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        dataset = Dataset.create_dataset(\n",
    "            dataset_config=config,\n",
    "            source=sample_file\n",
    "        )\n",
    "        print(f\"✓ Dataset created from file with ID: {dataset.dataset_config.id}\")\n",
    "        \n",
    "        # Download and verify\n",
    "        output_file = f\"file_output_{uuid.uuid4().hex[:8]}.csv\"\n",
    "        dataset.download(file_path=output_file)\n",
    "        \n",
    "        # Compare original and downloaded\n",
    "        original_df = pd.read_csv(sample_file)\n",
    "        downloaded_df = pd.read_csv(output_file)\n",
    "        print(f\"✓ Original shape: {original_df.shape}, Downloaded shape: {downloaded_df.shape}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        os.remove(sample_file)\n",
    "        os.remove(output_file)\n",
    "        dataset.delete()\n",
    "        print(\"✓ Cleanup completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating from file: {e}\")\n",
    "        # Cleanup on error\n",
    "        if os.path.exists(sample_file):\n",
    "            os.remove(sample_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_2_create_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_3_create_from_huggingface():\n",
    "    \"\"\"\n",
    "    Example 3: Creating Dataset from Hugging Face\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Creating dataset from Hugging Face dataset\n",
    "    - Working with different splits and row limits\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Example 3: Creating Dataset from Hugging Face ===\")\n",
    "    \n",
    "    unique_name = f\"hf_example_{uuid.uuid4().hex[:8]}\"\n",
    "    config = DatasetConfig(\n",
    "        name=unique_name,\n",
    "        model_type=ModelTypes.GENERATIVE_LLM\n",
    "    )\n",
    "    \n",
    "    # Configure Hugging Face dataset\n",
    "    hf_config = HuggingfaceDatasetConfig(\n",
    "        name=\"rungalileo/ragbench\",  # Stanford Question Answering Dataset\n",
    "        split=\"train\",\n",
    "        subset=\"covidqa\",\n",
    "        num_rows=10  # Limit to 10 rows for demo\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(\"Creating dataset from Hugging Face...\")\n",
    "        import time\n",
    "\n",
    "        # Create dataset with timeout for row addition\n",
    "        dataset = Dataset.create_dataset(\n",
    "            dataset_config=config,\n",
    "            source=hf_config\n",
    "        )\n",
    "        print(f\"✓ Dataset created from Hugging Face with ID: {dataset.dataset_config.id}\")\n",
    "        \n",
    "        try:\n",
    "            # Wait up to 10 minutes for rows to be added\n",
    "            timeout = time.time() + 600  # 10 minutes\n",
    "            while True:\n",
    "                try:\n",
    "                    # Download as DataFrame to inspect\n",
    "                    df = dataset.download(load_to_pandas=True)\n",
    "                    if isinstance(df, pd.DataFrame) and len(df) > 0 or time.time() > timeout:\n",
    "                        break\n",
    "                    print(\"Waiting for rows to be added...\")\n",
    "                    time.sleep(10)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while polling: {e}\")\n",
    "                    if time.time() > timeout:\n",
    "                        raise TimeoutError(\"Timed out waiting for rows to be added\")\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                    \n",
    "            print(f\"✓ Dataset shape: {df.shape}\")\n",
    "            print(\"Columns:\", list(df.columns))\n",
    "            print(\"Sample data:\")\n",
    "            print(df.head(2))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error while waiting for dataset rows: {e}\")\n",
    "            raise\n",
    "        print(\"✓ Dataset deleted\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating from Hugging Face: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_3_create_from_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_4_prompt_columns_and_evaluation():\n",
    "    \"\"\"\n",
    "    Example 4: Adding Prompt Columns and Evaluations\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Adding run prompt columns\n",
    "    - Adding evaluations\n",
    "    - Getting evaluation statistics\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Example 4: Prompt Columns and Evaluation ===\")\n",
    "    \n",
    "    unique_name = f\"eval_example_{uuid.uuid4().hex[:8]}\"\n",
    "    config = DatasetConfig(\n",
    "        name=unique_name,\n",
    "        model_type=ModelTypes.GENERATIVE_LLM\n",
    "    )\n",
    "    \n",
    "    dataset = Dataset(dataset_config=config)\n",
    "    \n",
    "    try:\n",
    "        # Create dataset and add initial data\n",
    "        dataset = dataset.create()\n",
    "        print(f\"✓ Dataset created: {dataset.dataset_config.name}\")\n",
    "        \n",
    "        # Add input columns\n",
    "        columns = [\n",
    "            Column(\n",
    "                name=\"user_question\",\n",
    "                data_type=DataTypeChoices.TEXT,\n",
    "                source=SourceChoices.OTHERS\n",
    "            ),\n",
    "            Column(\n",
    "                name=\"context\",\n",
    "                data_type=DataTypeChoices.TEXT,\n",
    "                source=SourceChoices.OTHERS\n",
    "            )\n",
    "        ]\n",
    "        dataset = dataset.add_columns(columns=columns)\n",
    "        \n",
    "        # Add sample data\n",
    "        rows = [\n",
    "            Row(\n",
    "                order=1,\n",
    "                cells=[\n",
    "                    Cell(column_name=\"user_question\", value=\"What is the capital of France?\"),\n",
    "                    Cell(column_name=\"context\", value=\"France is a country in Europe with Paris as its capital.\")\n",
    "                ]\n",
    "            ),\n",
    "            Row(\n",
    "                order=2,\n",
    "                cells=[\n",
    "                    Cell(column_name=\"user_question\", value=\"How do you make coffee?\"),\n",
    "                    Cell(column_name=\"context\", value=\"Coffee is made by brewing ground coffee beans with hot water.\")\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        dataset = dataset.add_rows(rows=rows)\n",
    "        print(\"✓ Initial data added\")\n",
    "        \n",
    "        # Add a run prompt column\n",
    "        print(\"Adding run prompt column...\")\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Answer questions based on the provided context.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Context: {{context}}\\n\\nQuestion: {{user_question}}\\n\\nAnswer:\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        dataset = dataset.add_run_prompt(\n",
    "            name=\"ai_response\",\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(\"✓ Run prompt column added\")\n",
    "        \n",
    "        # Add evaluation\n",
    "        print(\"Adding evaluation...\")\n",
    "        dataset = dataset.add_evaluation(\n",
    "            name=\"response_length_check\",\n",
    "            eval_template=\"LengthGreaterThan\",\n",
    "            required_keys_to_column_names={\n",
    "                \"text\": \"ai_response\"\n",
    "            },\n",
    "            config={\"min_length\": 10},\n",
    "            run=True\n",
    "        )\n",
    "        print(\"✓ Evaluation added\")\n",
    "        \n",
    "        # Get evaluation statistics\n",
    "        print(\"Getting evaluation statistics...\")\n",
    "        eval_stats = dataset.get_eval_stats()\n",
    "        print(f\"✓ Evaluation stats retrieved: {len(eval_stats)} metrics\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in prompt/evaluation example: {e}\")\n",
    "    finally:\n",
    "        try:\n",
    "            # dataset.delete()\n",
    "            print(\"✓ Dataset deleted\")\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_4_prompt_columns_and_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_5_optimization():\n",
    "    \"\"\"\n",
    "    Example 5: Dataset Optimization\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Setting up optimization for prompt templates\n",
    "    - Working with evaluation metrics for optimization\n",
    "    - Proper error handling and dataset cleanup\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Example 5: Dataset Optimization ===\")\n",
    "    \n",
    "    unique_name = f\"opt_example_{uuid.uuid4().hex[:8]}\"\n",
    "    config = DatasetConfig(\n",
    "        name=unique_name,\n",
    "        model_type=ModelTypes.GENERATIVE_LLM\n",
    "    )\n",
    "    \n",
    "    dataset = Dataset(dataset_config=config)\n",
    "    \n",
    "    try:\n",
    "        # Create and setup dataset\n",
    "        dataset = dataset.create()\n",
    "        if dataset.dataset_config:\n",
    "            print(f\"✓ Dataset created: {dataset.dataset_config.name}\")\n",
    "        else:\n",
    "            print(\"✗ Failed to create dataset - no configuration returned\")\n",
    "            return\n",
    "        \n",
    "        # Add columns using proper type casting\n",
    "        columns: List[Column] = [\n",
    "            Column(name=\"input_text\", data_type=DataTypeChoices.TEXT, source=SourceChoices.OTHERS),\n",
    "            Column(name=\"expected_output\", data_type=DataTypeChoices.TEXT, source=SourceChoices.OTHERS)\n",
    "        ]\n",
    "        dataset = dataset.add_columns(columns=columns)\n",
    "        print(\"✓ Columns added successfully\")\n",
    "        \n",
    "        # Add data using proper type casting\n",
    "        rows: List[Row] = [\n",
    "            Row(\n",
    "                order=1,\n",
    "                cells=[\n",
    "                    Cell(column_name=\"input_text\", value=\"Summarize this article about climate change\"),\n",
    "                    Cell(column_name=\"expected_output\", value=\"A concise summary focusing on key climate impacts\")\n",
    "                ]\n",
    "            ),\n",
    "            Row(\n",
    "                order=2,\n",
    "                cells=[\n",
    "                    Cell(column_name=\"input_text\", value=\"Explain quantum computing\"),\n",
    "                    Cell(column_name=\"expected_output\", value=\"A clear explanation suitable for beginners\")\n",
    "                ]\n",
    "            ),\n",
    "            Row(\n",
    "                order=3,\n",
    "                cells=[\n",
    "                    Cell(column_name=\"input_text\", value=\"Write a brief introduction to machine learning\"),\n",
    "                    Cell(column_name=\"expected_output\", value=\"An accessible introduction covering basic concepts\")\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        dataset = dataset.add_rows(rows=rows)\n",
    "        print(\"✓ Sample data added successfully\")\n",
    "        \n",
    "        # Add prompt column with proper message structure\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert assistant. Provide clear and helpful responses.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"{{input_text}}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(\"Adding run prompt column...\")\n",
    "        dataset = dataset.add_run_prompt(\n",
    "            name=\"optimized_response\",\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        print(\"✓ Run prompt column added successfully\")\n",
    "        \n",
    "        # Add evaluation for optimization - using a more reliable evaluation template\n",
    "        print(\"Adding evaluation for optimization...\")\n",
    "        dataset = dataset.add_evaluation(\n",
    "            name=\"response_quality_check\",\n",
    "            eval_template=\"LengthGreaterThan\",\n",
    "            required_keys_to_column_names={\n",
    "                \"text\": \"optimized_response\"\n",
    "            },\n",
    "            config={\"min_length\": 20},  # Ensure responses are at least 20 characters\n",
    "            run=True\n",
    "        )\n",
    "        print(\"✓ Evaluation added successfully\")\n",
    "        \n",
    "        # Wait a moment for evaluation to process\n",
    "        import time\n",
    "        print(\"Waiting for evaluation to process...\")\n",
    "        time.sleep(30)  # Increased wait time\n",
    "        \n",
    "        # Check if evaluations are available before optimization\n",
    "        print(\"Checking available evaluations...\")\n",
    "        try:\n",
    "            eval_stats = dataset.get_eval_stats()\n",
    "            print(f\"Available evaluations: {len(eval_stats) if isinstance(eval_stats, list) else 'Unknown'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not get evaluation stats: {e}\")\n",
    "        \n",
    "        # Setup optimization with corrected payload format\n",
    "        print(\"Setting up optimization...\")\n",
    "        try:\n",
    "            dataset = dataset.add_optimization(\n",
    "                optimization_name=\"prompt_optimization_example\",\n",
    "                prompt_column_name=\"optimized_response\",\n",
    "                optimize_type=\"PROMPT_TEMPLATE\",\n",
    "                model_config={\n",
    "                    \"model_name\": \"gpt-4o-mini\",  # Include model_name as required\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"max_tokens\": 300,\n",
    "                    \"frequency_penalty\": 0.0,\n",
    "                    \"presence_penalty\": 0.0,\n",
    "                    \"top_p\": 1.0\n",
    "                }\n",
    "            )\n",
    "        except DatasetError as opt_error:\n",
    "            print(f\"Optimization setup failed: {opt_error}\")\n",
    "            print(\"This might be due to:\")\n",
    "            print(\"1. Evaluations not fully processed yet\")\n",
    "            print(\"2. Column not properly linked to evaluations\")\n",
    "            print(\"3. Backend timing issues\")\n",
    "            print(\"Trying alternative approach...\")\n",
    "            \n",
    "            # Alternative: Try with a longer wait and retry\n",
    "            print(\"Waiting longer for backend processing...\")\n",
    "            time.sleep(10)\n",
    "            \n",
    "            dataset = dataset.add_optimization(\n",
    "                optimization_name=\"prompt_optimization_example\",\n",
    "                prompt_column_name=\"optimized_response\",\n",
    "                optimize_type=\"PROMPT_TEMPLATE\",\n",
    "                model_config={\n",
    "                    \"model_name\": \"gpt-4o-mini\",\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"max_tokens\": 300,\n",
    "                    \"frequency_penalty\": 0.0,\n",
    "                    \"presence_penalty\": 0.0,\n",
    "                    \"top_p\": 1.0\n",
    "                }\n",
    "            )\n",
    "        print(\"✓ Optimization setup completed successfully!\")\n",
    "        print(\"  - Optimization Name: prompt_optimization_example\")\n",
    "        print(\"  - Target Column: optimized_response\")\n",
    "        print(\"  - Optimization Type: PROMPT_TEMPLATE\")\n",
    "        print(\"  - Model Configuration: Updated with enhanced parameters\")\n",
    "        \n",
    "    except DatasetError as e:\n",
    "        print(f\"✗ Dataset Error in optimization example: {e}\")\n",
    "        print(\"  This might be due to missing evaluations or incorrect column references\")\n",
    "    except DatasetValidationError as e:\n",
    "        print(f\"✗ Validation Error in optimization example: {e}\")\n",
    "        print(\"  Check that all required fields are properly formatted\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Unexpected error in optimization example: {e}\")\n",
    "        print(f\"  Error type: {type(e).__name__}\")\n",
    "    finally:\n",
    "        try:\n",
    "            if dataset and dataset.dataset_config:\n",
    "                # dataset.delete()\n",
    "                print(\"✓ Dataset deleted successfully\")\n",
    "        except Exception as cleanup_error:\n",
    "            print(f\"✗ Error during cleanup: {cleanup_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_5_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_6_class_methods():\n",
    "    \"\"\"\n",
    "    Example 6: Using Class Methods for Simple Operations\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Using class methods for one-off operations\n",
    "    - Getting dataset configurations\n",
    "    - Simple download and delete operations\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Example 6: Class Methods for Simple Operations ===\")\n",
    "    \n",
    "    # First create a dataset to work with\n",
    "    unique_name = f\"class_methods_{uuid.uuid4().hex[:8]}\"\n",
    "    config = DatasetConfig(\n",
    "        name=unique_name,\n",
    "        model_type=ModelTypes.GENERATIVE_LLM\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Create using class method\n",
    "        dataset = Dataset.create_dataset(dataset_config=config)\n",
    "        print(f\"✓ Dataset created using class method: {dataset.dataset_config.name}\")\n",
    "        \n",
    "        # Add some data using class methods\n",
    "        columns = [\n",
    "            {\"name\": \"text\", \"data_type\": DataTypeChoices.TEXT},\n",
    "            {\"name\": \"score\", \"data_type\": DataTypeChoices.INTEGER}\n",
    "        ]\n",
    "        \n",
    "        Dataset.add_dataset_columns(\n",
    "            dataset_name=unique_name,\n",
    "            columns=columns\n",
    "        )\n",
    "        print(\"✓ Columns added using class method\")\n",
    "        \n",
    "        rows = [\n",
    "            {\n",
    "                \"cells\": [\n",
    "                    {\"column_name\": \"text\", \"value\": \"Sample text 1\"},\n",
    "                    {\"column_name\": \"score\", \"value\": 85}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"cells\": [\n",
    "                    {\"column_name\": \"text\", \"value\": \"Sample text 2\"},\n",
    "                    {\"column_name\": \"score\", \"value\": 92}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        Dataset.add_dataset_rows(\n",
    "            dataset_name=unique_name,\n",
    "            rows=rows\n",
    "        )\n",
    "        print(\"✓ Rows added using class method\")\n",
    "        \n",
    "        # Get dataset configuration\n",
    "        dataset_instance = Dataset.get_dataset_config(unique_name)\n",
    "        print(f\"✓ Retrieved dataset config: {dataset_instance.dataset_config.name}\")\n",
    "        \n",
    "        # Download using class method\n",
    "        output_file = f\"class_method_output_{uuid.uuid4().hex[:8]}.csv\"\n",
    "        Dataset.download_dataset(\n",
    "            dataset_name=unique_name,\n",
    "            file_path=output_file\n",
    "        )\n",
    "        print(f\"✓ Dataset downloaded using class method to {output_file}\")\n",
    "        \n",
    "        # Cleanup file\n",
    "        if os.path.exists(output_file):\n",
    "            os.remove(output_file)\n",
    "        \n",
    "        # Delete using class method\n",
    "        Dataset.delete_dataset(unique_name)\n",
    "        print(\"✓ Dataset deleted using class method\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in class methods example: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_6_class_methods()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
